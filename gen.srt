1
00:00:00,160 --> 00:00:03,058
♪ (music) ♪

2
00:00:07,068 --> 00:00:09,316
Alright, hi, everybody.

3
00:00:09,316 --> 00:00:12,218
I'm Alex, from the Brain Robotics team,

4
00:00:12,218 --> 00:00:15,787
and in this presentation, I'll be talking 
about how we use simulation

5
00:00:15,787 --> 00:00:19,721
and domain adaptation in some of
our real-world robot learning problems.

6
00:00:20,621 --> 00:00:24,457
So, first let me start by
introducing robot learning.

7
00:00:24,457 --> 00:00:28,036
The goal of robot learning
is to use machine learning

8
00:00:28,036 --> 00:00:29,826
to learn robotic skills

9
00:00:29,826 --> 00:00:31,995
that work in general environments.

10
00:00:31,995 --> 00:00:33,921
What we've seen so far is that

11
00:00:33,921 --> 00:00:35,731
if you control your environment a lot,

12
00:00:35,731 --> 00:00:38,634
you can get robots to do
pretty impressive things,

13
00:00:38,634 --> 00:00:40,168
and where techniques start to break down

14
00:00:40,168 --> 00:00:42,792
is when you try to apply
these same techniques

15
00:00:42,792 --> 00:00:44,651
to more general environments.

16
00:00:44,651 --> 00:00:47,214
And the thinking is that
if you use machine learning,

17
00:00:47,214 --> 00:00:48,934
then you can learn
from your environment,

18
00:00:48,934 --> 00:00:52,340
and this can help you address
these generalization issues.

19
00:00:52,340 --> 00:00:54,158
So, as a step in this direction,

20
00:00:54,158 --> 00:00:57,279
we've been looking at the problem
of robotic grasping.

21
00:00:57,279 --> 00:00:59,282
This is a project that we've been
working on

22
00:00:59,282 --> 00:01:01,783
in collaboration with some people at X.

23
00:01:01,783 --> 00:01:03,784
And to explain our problem setup a bit,

24
00:01:03,784 --> 00:01:05,551
we're going to have a real robot arm

25
00:01:05,551 --> 00:01:08,622
which is learning to pick up objects
out of a bin.

26
00:01:08,622 --> 00:01:11,655
There is going to be a camera looking down

27
00:01:11,655 --> 00:01:14,325
over the shoulder of the arm into the bin,

28
00:01:14,325 --> 00:01:17,760
and from this RGB image
we're going to train a neural network

29
00:01:17,760 --> 00:01:20,558
to learn what commands
it should send to the robot

30
00:01:20,558 --> 00:01:22,924
to successfully pick up objects.

31
00:01:22,924 --> 00:01:27,265
Now, we want to try to solve this task
using as few assumptions as possible.

32
00:01:27,265 --> 00:01:30,199
So, importantly, we're not going
to give any information

33
00:01:30,199 --> 00:01:33,499
about the geometry of what kinds
of objects we are trying to pick up,

34
00:01:33,499 --> 00:01:35,048
and we're also not going to give

35
00:01:35,048 --> 00:01:38,239
any information about
the depth of the scene.

36
00:01:38,239 --> 00:01:40,373
So in order to solve the task,

37
00:01:40,373 --> 00:01:42,577
the model needs to learn
hand-eye coordination

38
00:01:42,577 --> 00:01:45,212
or it needs to see where it is
within the camera image,

39
00:01:45,212 --> 00:01:48,015
and then figure out
where in the scene it is,

40
00:01:48,015 --> 00:01:50,729
and then combine these two to figure out
how it should move around.

41
00:01:51,869 --> 00:01:55,505
Now, in order to train this model,
we're going to need a lot of data

42
00:01:55,505 --> 00:01:57,666
because it's a pretty large scale
image model.

43
00:01:57,666 --> 00:02:02,822
And our solution at the time for this
was to simply use more robots.

44
00:02:02,822 --> 00:02:05,561
So this is what we call the "Arm Farm."

45
00:02:05,561 --> 00:02:08,691
These are six robots
collecting data in parallel.

46
00:02:08,691 --> 00:02:11,554
And if you have six robots,
you can collect data a lot faster

47
00:02:11,554 --> 00:02:13,261
than if you only have one robot.

48
00:02:13,261 --> 00:02:16,231
So using these robots,
we were able to collect

49
00:02:16,231 --> 00:02:18,697
over a million attempted grasps,

50
00:02:18,697 --> 00:02:20,923
over a total of thousands of robot hours,

51
00:02:20,923 --> 00:02:22,671
and then using this data we were able

52
00:02:22,671 --> 00:02:26,602
to successfully train models to learn
how to pick up objects.

53
00:02:26,602 --> 00:02:29,406
Now, this works,
but it still took a lot of time

54
00:02:29,406 --> 00:02:31,040
to collect this dataset.

55
00:02:31,040 --> 00:02:33,041
So this motivated looking into ways

56
00:02:33,041 --> 00:02:35,099
to reduce the amount 
of real-world data needed

57
00:02:35,099 --> 00:02:37,156
to learn these behaviors.

58
00:02:38,986 --> 00:02:41,656
One approach for doing this is simulation.

59
00:02:41,656 --> 00:02:43,314
So in the left video here,

60
00:02:43,314 --> 00:02:45,861
you can see the images that are
going into our model

61
00:02:45,861 --> 00:02:47,783
in our real world setup,

62
00:02:47,783 --> 00:02:49,785
and on the right here you can see

63
00:02:49,785 --> 00:02:52,700
our simulated recreation of that setup.

64
00:02:52,700 --> 00:02:55,222
Now, the advantage 
of moving things into simulation

65
00:02:55,222 --> 00:02:58,326
is that simulated robots
are a lot easier to scale.

66
00:02:58,326 --> 00:03:01,460
We've been able to spin up
thousands of simulated robots

67
00:03:01,460 --> 00:03:03,328
grasping various objects,

68
00:03:03,328 --> 00:03:06,436
and using this setup we were able 
to collect millions of grasps

69
00:03:06,436 --> 00:03:08,127
in just over eight hours,

70
00:03:08,127 --> 00:03:11,008
instead of the weeks that were required
for our original dataset.

71
00:03:12,836 --> 00:03:15,931
Now, this is good 
for getting a lot of data,

72
00:03:15,931 --> 00:03:18,535
but unfortunately models
trained in simulation

73
00:03:18,535 --> 00:03:21,751
tend not to transfer
to the actual real world robot.

74
00:03:21,751 --> 00:03:24,707
There are a lot of systematic differences
between the two.

75
00:03:24,707 --> 00:03:28,020
One big one is the visual
appearances of different things.

76
00:03:28,020 --> 00:03:30,346
And another big one is just
physical differences

77
00:03:30,346 --> 00:03:31,943
between our real-world physics

78
00:03:31,943 --> 00:03:34,179
and our simulated physics.

79
00:03:34,179 --> 00:03:37,181
So what we did was,
we were able to very quickly

80
00:03:37,181 --> 00:03:41,383
train our model on simulation
to get to around 90% grasp success.

81
00:03:41,383 --> 00:03:43,649
We then deployed to the real robot,

82
00:03:43,649 --> 00:03:46,397
and it succeeds just over 20% of the time,

83
00:03:46,397 --> 00:03:48,419
which is a very big performance drop.

84
00:03:49,039 --> 00:03:51,520
So in order to actually
get good performance,

85
00:03:51,520 --> 00:03:53,841
we need to do something
a bit more clever.

86
00:03:53,841 --> 00:03:56,690
So this motivated looking 
into Sim-to-Real transfer,

87
00:03:56,690 --> 00:03:59,363
which is a set of
transfer-learning techniques

88
00:03:59,363 --> 00:04:01,264
for trying to use simulated data

89
00:04:01,264 --> 00:04:04,499
to improve your
real-world sample efficiency.

90
00:04:06,549 --> 00:04:09,969
Now, there are a few different ways
you can do this.

91
00:04:09,969 --> 00:04:11,634
One approach for doing this is

92
00:04:11,634 --> 00:04:14,035
adding more randomization
into your simulator.

93
00:04:14,035 --> 00:04:16,469
You can do this by changing
around the textures

94
00:04:16,469 --> 00:04:17,979
that you apply to different objects,

95
00:04:17,979 --> 00:04:19,953
changing around their colors,

96
00:04:19,953 --> 00:04:22,945
changing how lighting is interacting
with your scene,

97
00:04:22,945 --> 00:04:27,113
and you can also play around with changing
the geometry of what kinds of objects

98
00:04:27,113 --> 00:04:28,565
you're trying to pick up.

99
00:04:28,565 --> 00:04:31,946
Another way of doing this
is domain adaptation,

100
00:04:31,946 --> 00:04:34,252
which is a set of techniques for learning

101
00:04:34,252 --> 00:04:38,121
when you have two domains of data
that have some common structure,

102
00:04:38,121 --> 00:04:39,857
but are still somewhat different.

103
00:04:39,857 --> 00:04:42,730
In our case the two domains are
going to be our simulated robot data

104
00:04:42,730 --> 00:04:45,303
and our real robot data.

105
00:04:45,303 --> 00:04:47,456
And there are feature-level
ways of doing this

106
00:04:47,456 --> 00:04:49,825
and there are pixel-level
ways of doing this.

107
00:04:49,825 --> 00:04:53,134
Now, in this work, we tried
all of these approaches,

108
00:04:53,134 --> 00:04:55,678
and in this presentation,
I'm going to focus primarily

109
00:04:55,678 --> 00:04:58,113
on the domain adaptation side of things.

110
00:05:00,503 --> 00:05:02,937
So, in feature-level domain adaptation

111
00:05:02,937 --> 00:05:06,437
what we're going to do is we're going 
to take our simulated data,

112
00:05:06,437 --> 00:05:07,804
take our real data,

113
00:05:07,804 --> 00:05:10,670
train the same model on both datasets,

114
00:05:10,670 --> 00:05:13,108
but then at an intermediate
feature layer of the network,

115
00:05:13,108 --> 00:05:15,647
we're going to attach a similarity loss.

116
00:05:15,647 --> 00:05:19,483
And the similarity loss is going to
encourage the distribution of features

117
00:05:19,483 --> 00:05:21,530
to be the same across both domains.

118
00:05:21,530 --> 00:05:24,938
Now, one approach for doing this
which has worked well recently

119
00:05:24,938 --> 00:05:27,755
is called Domain-Adversarial
Neural Networks.

120
00:05:27,755 --> 00:05:31,422
And the way these work is that the
similarity loss is implemented

121
00:05:31,422 --> 00:05:34,725
as a small neural net that tries
to predict the domain

122
00:05:34,725 --> 00:05:37,410
based on the input features 
it's receiving,

123
00:05:37,410 --> 00:05:39,123
and then the rest of the model is trying

124
00:05:39,123 --> 00:05:42,243
to confuse this domain classifier
as much as possible.

125
00:05:44,673 --> 00:05:48,066
Now, pixel-level methods try to work
at the problem

126
00:05:48,066 --> 00:05:49,701
from a different point of view.

127
00:05:49,701 --> 00:05:52,703
Instead of trying to learn
domain invariant features,

128
00:05:52,703 --> 00:05:55,767
we're going to try to transform
our images at the pixel level

129
00:05:55,767 --> 00:05:57,507
to look more realistic.

130
00:05:57,507 --> 00:06:01,073
So what we do here is we take 
a generative-adversarial network;

131
00:06:01,073 --> 00:06:04,777
we feed it an image from our simulator,

132
00:06:04,777 --> 00:06:08,578
and then it's going to output an image
that looks more realistic.

133
00:06:08,578 --> 00:06:11,514
And then we're going to use the output
of this generator

134
00:06:11,514 --> 00:06:15,411
to train whatever task model
that we want to train.

135
00:06:15,411 --> 00:06:16,952
Now we're going to train both

136
00:06:16,952 --> 00:06:19,354
the generator and the task model
at the same time.

137
00:06:19,354 --> 00:06:21,387
We found that in practice, this was useful

138
00:06:21,387 --> 00:06:23,821
because it helps ground
the generator output

139
00:06:23,821 --> 00:06:27,082
to be useful for actually training
your downstream task.

140
00:06:29,232 --> 00:06:31,927
Alright. So taking a step back,

141
00:06:31,927 --> 00:06:36,001
feature-level methods can learn
domain-invariant features

142
00:06:36,001 --> 00:06:38,100
when you have data from related domains

143
00:06:38,100 --> 00:06:40,269
that aren't quite identical.

144
00:06:40,269 --> 00:06:43,602
Meanwhile, pixel-level methods
can transform your data

145
00:06:43,602 --> 00:06:46,403
to look more like your real-world data,

146
00:06:46,403 --> 00:06:48,138
but in practice they don't work perfectly,

147
00:06:48,138 --> 00:06:50,205
and there are still some small artifacts

148
00:06:50,205 --> 00:06:52,743
and inaccuracies from
the generator output.

149
00:06:52,743 --> 00:06:56,712
So our thinking went, "Why don't we simply
combine both of these approaches?"

150
00:06:56,712 --> 00:06:59,191
We can apply a pixel-level method

151
00:06:59,191 --> 00:07:01,913
to try to transform the data
as much as possible,

152
00:07:01,913 --> 00:07:04,250
and this isn't going to get us
all the way there,

153
00:07:04,250 --> 00:07:06,951
but then we can attach a
feature-level method on top of this

154
00:07:06,951 --> 00:07:09,915
to try to close 
the reality gap even further,

155
00:07:09,915 --> 00:07:12,508
and combined these form what we call
the grasp gen

156
00:07:12,508 --> 00:07:13,966
which is a combination of both

157
00:07:13,966 --> 00:07:16,222
pixel-level and feature-level
domain adaptation.

158
00:07:16,222 --> 00:07:18,257
In the left half of the video here

159
00:07:18,257 --> 00:07:19,857
you can see a simulated grasp.

160
00:07:19,857 --> 00:07:22,861
In the right half you can see 
the output of our generator.

161
00:07:22,861 --> 00:07:25,228
And you can see that it's
learning some pretty cool things

162
00:07:25,228 --> 00:07:27,730
in terms of drawing
what the tray should look like,

163
00:07:27,730 --> 00:07:30,333
drawing more realistic textures
on the arm,

164
00:07:30,333 --> 00:07:32,235
drawing shadows 
that the objects are casting.

165
00:07:32,235 --> 00:07:34,377
It's also learned how to even draw shadows

166
00:07:34,377 --> 00:07:36,435
as the arm is moving around in the scene.

167
00:07:36,435 --> 00:07:38,403
And it certainly isn't perfect.

168
00:07:38,403 --> 00:07:41,805
There are still these little
odd splotches of color around,

169
00:07:41,805 --> 00:07:43,808
but it's definitely learning something

170
00:07:43,808 --> 00:07:47,028
about what it means for an image 
to look more realistic.

171
00:07:47,028 --> 00:07:51,147
Now, this is good for getting
a lot of pretty images,

172
00:07:51,147 --> 00:07:54,813
but what matters for our problem is
whether these images are actually useful

173
00:07:54,813 --> 00:07:58,300
for reducing them 
onto real-world data required.

174
00:07:58,960 --> 00:08:01,417
And we find that it does.

175
00:08:01,417 --> 00:08:03,386
So, to explain this chart a bit:

176
00:08:03,386 --> 00:08:07,136
On the x-axis is the number
of real-world samples used,

177
00:08:07,136 --> 00:08:09,646
and we compared the performance
of different methods

178
00:08:09,646 --> 00:08:13,057
as we vary them onto real-world data
given to the model.

179
00:08:13,057 --> 00:08:16,727
The blue bar is our performance
when we use only simulated data.

180
00:08:16,727 --> 00:08:20,129
The red bar is our performance
when we use only real data,

181
00:08:20,129 --> 00:08:24,297
and the orange bar is our performance
when we use both simulated and real data

182
00:08:24,297 --> 00:08:27,667
and the domain adaptation methods
that I've been talking about.

183
00:08:27,667 --> 00:08:30,611
And what we found is that
when we use just 2%

184
00:08:30,611 --> 00:08:32,801
of our original real-world dataset,

185
00:08:32,801 --> 00:08:35,470
and we apply domain adaptation to it,

186
00:08:35,470 --> 00:08:37,696
we're able to get
the same level of performance

187
00:08:37,696 --> 00:08:41,238
so this reduces the number 
of real-world samples we needed

188
00:08:41,238 --> 00:08:43,366
by up to 50 times,
which is really exciting

189
00:08:43,366 --> 00:08:46,515
in terms of not needing to run
robots for a large amount of time

190
00:08:46,515 --> 00:08:48,749
to learn these grasping behaviors.

191
00:08:48,749 --> 00:08:52,149
Additionally, we found 
that even when we give

192
00:08:52,149 --> 00:08:54,652
all of the real-world data to the model,

193
00:08:54,652 --> 00:08:56,586
when we give simulated data as well,

194
00:08:56,586 --> 00:08:59,055
we're still able to see
improved performance

195
00:08:59,055 --> 00:09:01,589
so that implies that we haven't hit
the data capacity limits

196
00:09:01,589 --> 00:09:03,824
for this grasping problem.

197
00:09:03,824 --> 00:09:07,561
And finally, there's a way
to train this setup

198
00:09:07,561 --> 00:09:09,859
without having real-world labels,

199
00:09:09,859 --> 00:09:11,762
and when we trained 
the model in this setting,

200
00:09:11,762 --> 00:09:14,665
we found that we were still able to get
pretty good performance

201
00:09:14,665 --> 00:09:17,031
on the real-world robot.

202
00:09:19,091 --> 00:09:21,536
Now, this was the work of a large team

203
00:09:21,536 --> 00:09:24,170
across both Brain as well as X.

204
00:09:24,170 --> 00:09:26,406
I'd like to thank all of my collaborators.

205
00:09:26,406 --> 00:09:28,641
Here's a link to the original paper.

206
00:09:28,641 --> 00:09:31,341
And I believe there is also a blog post,

207
00:09:31,341 --> 00:09:34,565
if people are interested
in hearing more details.

208
00:09:34,565 --> 00:09:35,675
Thanks.

209
00:09:35,675 --> 00:09:38,119
(applause)

210
00:09:38,781 --> 00:09:41,928
♪ (music) ♪

