1
00:00:07,068 --> 00:00:09,316
大家好  
Alright, hi, everybody.

2
00:00:09,316 --> 00:00:12,218
我是来自 Brain Robotics 团队的 Alex  
I'm Alex, from the Brain Robotics team,

3
00:00:12,218 --> 00:00:15,787
在本演示中  我将谈论我们如何在一些
and in this presentation, I'll be talking about how we use simulation

4
00:00:15,787 --> 00:00:19,721
现实世界中的机器人学习问题中使用模拟和域适应  
and domain adaptation in some of our real-world robot learning problems.

5
00:00:20,621 --> 00:00:24,457
所以  首先让我开始介绍机器人学习  
So, first let me start by introducing robot learning.

6
00:00:24,457 --> 00:00:28,036
机器人学习的目标是使用机器学习
The goal of robot learning is to use machine learning

7
00:00:28,036 --> 00:00:29,826
来学习在一般环境中能运作的机器人技能  
to learn robotic skills

8
00:00:29,826 --> 00:00:31,790
来学习在一般环境中能运作的机器人技能  
that work in general environments.

9
00:00:31,995 --> 00:00:33,921
到目前为止  我们看到的是  
What we've seen so far is that

10
00:00:33,921 --> 00:00:35,731
如果你常常控制你的环境  
if you control your environment a lot,

11
00:00:35,731 --> 00:00:38,634
你可以让机器人做出很厉害的事情  
you can get robots to do pretty impressive things,

12
00:00:38,634 --> 00:00:40,168
而技术开始崩溃的地方
and where techniques start to break down

13
00:00:40,168 --> 00:00:42,792
是当你尝试将这些相同的技术应用
is when you try to apply these same techniques

14
00:00:42,792 --> 00:00:44,651
到更一般的环境时  
to more general environments.

15
00:00:44,651 --> 00:00:47,214
他们的思想是  如果你使用机器学习  
And the thinking is that if you use machine learning,

16
00:00:47,214 --> 00:00:48,934
那么你可以从你的环境中学习  
then you can learn from your environment,

17
00:00:48,934 --> 00:00:52,340
这可以帮助你解决这些泛化问题  
and this can help you address these generalization issues.

18
00:00:52,340 --> 00:00:54,158
所以  作为朝这个方向迈出的一步  
So, as a step in this direction,

19
00:00:54,158 --> 00:00:57,279
我们一直在关注机器人抓取的问题  
we've been looking at the problem of robotic grasping.

20
00:00:57,279 --> 00:00:59,282
这是我们一直在与X的一些人合作的一个项目  
This is a project that we've been working on

21
00:00:59,282 --> 00:01:01,518
这是我们一直在与X的一些人合作的一个项目  
in collaboration with some people at X.

22
00:01:01,783 --> 00:01:03,784
为了解释我们的问题设置  
And to explain our problem setup a bit,

23
00:01:03,784 --> 00:01:05,551
我们将有一个真正的机器人手臂  
we're going to have a real robot arm

24
00:01:05,551 --> 00:01:08,622
它正在学习从垃圾箱中拾取物体  
which is learning to pick up objects out of a bin.

25
00:01:08,622 --> 00:01:11,655
将会有一台摄像机俯视
There is going to be a camera looking down

26
00:01:11,655 --> 00:01:14,325
手臂的肩膀进入垃圾箱  
over the shoulder of the arm into the bin,

27
00:01:14,325 --> 00:01:17,760
从这张 RGB 图像中  我们将训练一个神经网络  
and from this RGB image we're going to train a neural network

28
00:01:17,760 --> 00:01:20,558
以了解它应该发送给机器人的命令
to learn what commands it should send to the robot

29
00:01:20,558 --> 00:01:22,924
以成功拾取物体  
to successfully pick up objects.

30
00:01:22,924 --> 00:01:27,265
现在  我们想尽量使用尽可能少的假设来解决这个任务  
Now, we want to try to solve this task using as few assumptions as possible.

31
00:01:27,265 --> 00:01:30,199
所以  重要的是  我们不会提供
So, importantly, we're not going to give any information

32
00:01:30,199 --> 00:01:33,499
关于我们想要拾取什么类型的物体的任何信息  
about the geometry of what kinds of objects we are trying to pick up,

33
00:01:33,499 --> 00:01:35,048
我们也不会提供
and we're also not going to give

34
00:01:35,048 --> 00:01:38,239
有关场景深度的任何信息  
any information about the depth of the scene.

35
00:01:38,239 --> 00:01:40,373
所以为了解决这个任务  
So in order to solve the task,

36
00:01:40,373 --> 00:01:42,577
模型需要学习手眼协调  
the model needs to learn hand-eye coordination

37
00:01:42,577 --> 00:01:45,212
或者需要知道它在摄像机图像中的位置  
or it needs to see where it is within the camera image,

38
00:01:45,212 --> 00:01:48,015
然后找出它在场景中的位置  
and then figure out where in the scene it is,

39
00:01:48,015 --> 00:01:50,729
然后结合这两者来弄清楚它应该如何移动  
and then combine these two to figure out how it should move around.

40
00:01:51,869 --> 00:01:55,505
现在  为了训练这个模型  我们将需要大量的数据  
Now, in order to train this model, we're going to need a lot of data

41
00:01:55,505 --> 00:01:57,666
因为它是一个相当大规模的图像模型  
because it's a pretty large scale image model.

42
00:01:57,666 --> 00:02:02,822
而我们当时的解决方案是使用更多的机器人  
And our solution at the time for this was to simply use more robots.

43
00:02:02,822 --> 00:02:05,561
所以这就是我们所说的  手臂农场    
So this is what we call the "Arm Farm."

44
00:02:05,561 --> 00:02:08,691
有六台机器人并行采集数据  
These are six robots collecting data in parallel.

45
00:02:08,691 --> 00:02:11,554
如果你有六个机器人  你可以更快地收集数据  
And if you have six robots, you can collect data a lot faster

46
00:02:11,554 --> 00:02:13,261
如果跟只有一个机器人相比  
than if you only have one robot.

47
00:02:13,261 --> 00:02:16,231
因此  使用这些机器人  我们能够在数千个机器人小时内收集超过一百万次抓握的尝试  
So using these robots, we were able to collect

48
00:02:16,231 --> 00:02:18,697
因此  使用这些机器人  我们能够在数千个机器人小时内收集超过一百万次抓握的尝试  
over a million attempted grasps,

49
00:02:18,697 --> 00:02:20,718
因此  使用这些机器人  我们能够在数千个机器人小时内收集超过一百万次抓握的尝试  
over a total of thousands of robot hours,

50
00:02:20,923 --> 00:02:22,671
然后使用这些数据  我们能够
and then using this data we were able

51
00:02:22,671 --> 00:02:26,602
成功地训练模型学习如何拾取物体  
to successfully train models to learn how to pick up objects.

52
00:02:26,602 --> 00:02:29,406
这是可行  但仍需要很长时间
Now, this works, but it still took a lot of time

53
00:02:29,406 --> 00:02:31,040
才能收集此数据集  
to collect this dataset.

54
00:02:31,040 --> 00:02:33,041
因此  这一点激励我们寻找方法
So this motivated looking into ways

55
00:02:33,041 --> 00:02:35,099
减少学习这些行为所需的实际数据量  
to reduce the amount of real-world data needed

56
00:02:35,099 --> 00:02:38,858
减少学习这些行为所需的实际数据量  
to learn these behaviors.

57
00:02:38,986 --> 00:02:41,656
一种做法是模拟  
One approach for doing this is simulation.

58
00:02:41,656 --> 00:02:43,314
因此  在左侧视频中  
So in the left video here,

59
00:02:43,314 --> 00:02:45,861
你可以看到在我们的真实世界设置中进入我们模型的图像  
you can see the images that are going into our model

60
00:02:45,861 --> 00:02:47,534
你可以看到在我们的真实世界设置中进入我们模型的图像  
in our real world setup,

61
00:02:47,783 --> 00:02:49,785
而在右侧  你可以看到
and on the right here you can see

62
00:02:49,785 --> 00:02:52,700
我们模拟该设置的重新创建  
our simulated recreation of that setup.

63
00:02:52,700 --> 00:02:55,222
现在  将事物转化为模拟的优势在于
Now, the advantage of moving things into simulation

64
00:02:55,222 --> 00:02:58,326
模拟机器人更容易扩展  
is that simulated robots are a lot easier to scale.

65
00:02:58,326 --> 00:03:01,460
我们已经能够旋转数以千计的模拟机器人  
We've been able to spin up thousands of simulated robots

66
00:03:01,460 --> 00:03:03,328
它们会抓住各种物体
grasping various objects,

67
00:03:03,328 --> 00:03:06,436
使用这种设置  我们能够在短短八个多小时内收集数百万的抓握  
and using this setup we were able to collect millions of grasps

68
00:03:06,436 --> 00:03:07,910
使用这种设置  我们能够在短短八个多小时内收集数百万的抓握  
in just over eight hours,

69
00:03:08,127 --> 00:03:11,008
而不是原始数据集所需的几周  
instead of the weeks that were required for our original dataset.

70
00:03:12,836 --> 00:03:15,931
这对于获取大量数据非常有用  
Now, this is good for getting a lot of data,

71
00:03:15,931 --> 00:03:18,535
但不幸的是  模拟培训的模型
but unfortunately models trained in simulation

72
00:03:18,535 --> 00:03:21,751
往往不会转移到真实世界的机器人  
tend not to transfer to the actual real world robot.

73
00:03:21,751 --> 00:03:24,707
两者之间存在很多系统性差异  
There are a lot of systematic differences between the two.

74
00:03:24,707 --> 00:03:28,020
其中一件大事是不同事物的视觉外观  
One big one is the visual appearances of different things.

75
00:03:28,020 --> 00:03:30,346
而另一个重要的就是我们现实世界的物理学和我们的模拟物理学之间的物理差异  
And another big one is just physical differences

76
00:03:30,346 --> 00:03:31,943
而另一个重要的就是我们现实世界的物理学和我们的模拟物理学之间的物理差异  
between our real-world physics

77
00:03:31,943 --> 00:03:34,060
而另一个重要的就是我们现实世界的物理学和我们的模拟物理学之间的物理差异  
and our simulated physics.

78
00:03:34,179 --> 00:03:37,181
所以我们所做的就是  我们能够快速
So what we did was, we were able to very quickly

79
00:03:37,181 --> 00:03:41,383
训练我们的模拟模型  以获得大约90％的抓握成功  
train our model on simulation to get to around 90% grasp success.

80
00:03:41,383 --> 00:03:43,649
然后  我们部署到真正的机器人上  
We then deployed to the real robot,

81
00:03:43,649 --> 00:03:46,397
它的成功率超过20％  
and it succeeds just over 20% of the time,

82
00:03:46,397 --> 00:03:48,419
这是一个非常大的性能下降  
which is a very big performance drop.

83
00:03:49,039 --> 00:03:51,520
所以为了获得好的表现  
So in order to actually get good performance,

84
00:03:51,520 --> 00:03:53,841
我们的做法需要更聪明一些  
we need to do something a bit more clever.

85
00:03:53,841 --> 00:03:56,690
所以这个动机考虑了模拟 - 到-现实的转换  
So this motivated looking into Sim-to-Real transfer,

86
00:03:56,690 --> 00:03:59,363
这是一组转换学习技术  
which is a set of transfer-learning techniques

87
00:03:59,363 --> 00:04:01,264
用于尝试使用模拟数据
for trying to use simulated data

88
00:04:01,264 --> 00:04:04,499
来提高你的实际样本效率  
to improve your real-world sample efficiency.

89
00:04:06,549 --> 00:04:09,969
有几种不同的方法可以做到这一点  
Now, there are a few different ways you can do this.

90
00:04:09,969 --> 00:04:11,634
这样做的一种方法是在
One approach for doing this is

91
00:04:11,634 --> 00:04:14,035
你的模拟器中增加更多的随机化  
adding more randomization into your simulator.

92
00:04:14,035 --> 00:04:16,469
你可以通过更改应用于不同物体的纹理  
You can do this by changing around the textures

93
00:04:16,469 --> 00:04:17,929
你可以通过更改应用于不同物体的纹理  
that you apply to different objects,

94
00:04:17,979 --> 00:04:19,953
改变其颜色  
changing around their colors,

95
00:04:19,953 --> 00:04:22,945
改变灯光与场景的交互方式  
changing how lighting is interacting with your scene,

96
00:04:22,945 --> 00:04:27,113
以及改变所要尝试捡起的物体的几何形状来实现这一点  
and you can also play around with changing the geometry of what kinds of objects

97
00:04:27,113 --> 00:04:28,354
以及改变所要尝试捡起的物体的几何形状来实现这一点  
you're trying to pick up.

98
00:04:28,565 --> 00:04:31,946
另一种方法是领域适应  
Another way of doing this is domain adaptation,

99
00:04:31,946 --> 00:04:34,252
这是一组学习技术  
which is a set of techniques for learning

100
00:04:34,252 --> 00:04:38,121
这适用于当你有一些共同的结构的两个数据领域  
when you have two domains of data that have some common structure,

101
00:04:38,121 --> 00:04:39,857
但仍然有些不同  
but are still somewhat different.

102
00:04:39,857 --> 00:04:42,730
在我们的情况下  这两个领域将是我们的模拟机器人数据
In our case the two domains are going to be our simulated robot data

103
00:04:42,730 --> 00:04:45,303
和我们的真实机器人数据  
and our real robot data.

104
00:04:45,303 --> 00:04:47,456
还有一些功能级别的方法可以做到这一点  
And there are feature-level ways of doing this

105
00:04:47,456 --> 00:04:49,825
并且也有像素级别的方法可以做到这一点  
and there are pixel-level ways of doing this.

106
00:04:49,825 --> 00:04:53,134
现在  在这项工作中  我们尝试了所有这些方法  
Now, in this work, we tried all of these approaches,

107
00:04:53,134 --> 00:04:55,678
并且在本演示中  我将首先关注
and in this presentation, I'm going to focus primarily

108
00:04:55,678 --> 00:04:58,113
领域适应这方面  
on the domain adaptation side of things.

109
00:05:00,503 --> 00:05:02,937
因此  在特征级别领域培训中  
So, in feature-level domain adaptation

110
00:05:02,937 --> 00:05:06,437
我们要做的是我们将采用我们的模拟数据  
what we're going to do is we're going to take our simulated data,

111
00:05:06,437 --> 00:05:07,804
拿出我们的真实数据  
take our real data,

112
00:05:07,804 --> 00:05:10,670
在两个数据集上训练相同的模型  
train the same model on both datasets,

113
00:05:10,670 --> 00:05:13,108
然后在网络的中间特征层  
but then at an intermediate feature layer of the network,

114
00:05:13,108 --> 00:05:15,647
我们将附上相似性损失  
we're going to attach a similarity loss.

115
00:05:15,647 --> 00:05:19,483
而相似性损失将会促使特征的分布
And the similarity loss is going to encourage the distribution of features

116
00:05:19,483 --> 00:05:21,530
在整个领域变得相同  
to be the same across both domains.

117
00:05:21,530 --> 00:05:24,938
一种很好的方法
Now, one approach for doing this which has worked well recently

118
00:05:24,938 --> 00:05:27,755
被称之为领域 - 对抗神经网络  
is called Domain-Adversarial Neural Networks.

119
00:05:27,755 --> 00:05:31,422
它的运作的方式是将相似性损失
And the way these work is that the similarity loss is implemented

120
00:05:31,422 --> 00:05:34,725
作为一个小型的神经网络来实现  它试图根据它所接收到的输入特征来预测领域  
as a small neural net that tries to predict the domain

121
00:05:34,725 --> 00:05:37,240
作为一个小型的神经网络来实现  它试图根据它所接收到的输入特征来预测领域  
based on the input features it's receiving,

122
00:05:37,410 --> 00:05:39,123
然后其余的模型试图尽可能地
and then the rest of the model is trying

123
00:05:39,123 --> 00:05:42,243
混淆这个领域分类器  
to confuse this domain classifier as much as possible.

124
00:05:44,673 --> 00:05:48,066
像素级别的方法尝试从不同的角度解决问题  
Now, pixel-level methods try to work at the problem

125
00:05:48,066 --> 00:05:49,552
像素级别的方法尝试从不同的角度解决问题  
from a different point of view.

126
00:05:49,701 --> 00:05:52,703
我们尝试让像素级别的图像看起来更逼真  而不是试图学习领域不变特征
Instead of trying to learn domain invariant features,

127
00:05:52,703 --> 00:05:55,767
我们尝试让像素级别的图像看起来更逼真  而不是试图学习领域不变特征
we're going to try to transform our images at the pixel level

128
00:05:55,767 --> 00:05:57,336
我们尝试让像素级别的图像看起来更逼真  而不是试图学习领域不变特征
to look more realistic.

129
00:05:57,507 --> 00:06:01,073
所以我们在这里的做法是我们采取一个生成对抗网络;
So what we do here is we take a generative-adversarial network;

130
00:06:01,073 --> 00:06:04,777
我们从模拟器中提供一张图像  
we feed it an image from our simulator,

131
00:06:04,777 --> 00:06:08,578
然后输出看起来更逼真的图像  
and then it's going to output an image that looks more realistic.

132
00:06:08,578 --> 00:06:11,514
然后我们将使用这个生成器的输出
And then we're going to use the output of this generator

133
00:06:11,514 --> 00:06:15,411
来训练我们想要训练的任何任务模型  
to train whatever task model that we want to train.

134
00:06:15,411 --> 00:06:16,952
现在我们要同时训练
Now we're going to train both

135
00:06:16,952 --> 00:06:19,354
发电机和任务模型  
the generator and the task model at the same time.

136
00:06:19,354 --> 00:06:21,387
我们发现在实践中  这很有用  
We found that in practice, this was useful

137
00:06:21,387 --> 00:06:23,821
因为它有助于将发电机输出信号
because it helps ground the generator output

138
00:06:23,821 --> 00:06:27,082
用于实际训练下游任务  
to be useful for actually training your downstream task.

139
00:06:29,232 --> 00:06:31,927
好的  退一步说  
Alright. So taking a step back,

140
00:06:31,927 --> 00:06:36,001
功能级别的方法可以在你有不完全相同的相关领域的数据时学习域不变特征  
feature-level methods can learn domain-invariant features

141
00:06:36,001 --> 00:06:38,100
功能级别的方法可以在你有不完全相同的相关领域的数据时学习域不变特征  
when you have data from related domains

142
00:06:38,100 --> 00:06:39,990
功能级别的方法可以在你有不完全相同的相关领域的数据时学习域不变特征  
that aren't quite identical.

143
00:06:40,269 --> 00:06:43,602
同时  像素级方法可以将数据
Meanwhile, pixel-level methods can transform your data

144
00:06:43,602 --> 00:06:46,403
转换为更接近真实世界的数据  
to look more like your real-world data,

145
00:06:46,403 --> 00:06:48,138
但实际上它们的运作不是完美的  
but in practice they don't work perfectly,

146
00:06:48,138 --> 00:06:50,205
并且发生器输出中仍然存在一些小的瑕疵和不准确性  
and there are still some small artifacts

147
00:06:50,205 --> 00:06:52,337
并且发生器输出中仍然存在一些小的瑕疵和不准确性  
and inaccuracies from the generator output.

148
00:06:52,743 --> 00:06:56,712
所以我们的想法是    为什么我们不把这两种方法结合起来    
So our thinking went, "Why don't we simply combine both of these approaches?"

149
00:06:56,712 --> 00:06:59,191
我们可以应用像素级别的方法
We can apply a pixel-level method

150
00:06:59,191 --> 00:07:01,913
尽可能多地转换数据  
to try to transform the data as much as possible,

151
00:07:01,913 --> 00:07:04,250
但这不会让我们一路畅通无阻  
and this isn't going to get us all the way there,

152
00:07:04,250 --> 00:07:06,951
但是随后可以在此基础上附加一个功能级别的方法  
but then we can attach a feature-level method on top of this

153
00:07:06,951 --> 00:07:09,915
以尝试进一步缩小现实的差距  
to try to close the reality gap even further,

154
00:07:09,915 --> 00:07:12,508
并将这些形式组合在一起  形成我们所说的抓取 gen  
and combined these form what we call the grasp gen

155
00:07:12,508 --> 00:07:13,966
它是像素级和特征级域适应的组合  
which is a combination of both

156
00:07:13,966 --> 00:07:16,144
它是像素级和特征级域适应的组合  
pixel-level and feature-level domain adaptation.

157
00:07:16,222 --> 00:07:18,257
在这视频的左半部分  
In the left half of the video here

158
00:07:18,257 --> 00:07:19,857
你可以看到一个模拟的抓握  
you can see a simulated grasp.

159
00:07:19,857 --> 00:07:22,861
在右半边你可以看到我们的发电机的输出  
In the right half you can see the output of our generator.

160
00:07:22,861 --> 00:07:25,228
你可以看到它正在学习一些非常酷的东西  
And you can see that it's learning some pretty cool things

161
00:07:25,228 --> 00:07:27,730
从绘制托盘应该看起来的样子  
in terms of drawing what the tray should look like,

162
00:07:27,730 --> 00:07:30,333
在手臂上绘制更逼真的纹理  
drawing more realistic textures on the arm,

163
00:07:30,333 --> 00:07:32,235
绘制物体正在投射的阴影  
drawing shadows that the objects are casting.

164
00:07:32,235 --> 00:07:34,377
它还学会了当手臂在场景中移动时  如何在手臂上绘制阴影  
It's also learned how to even draw shadows

165
00:07:34,377 --> 00:07:36,364
它还学会了当手臂在场景中移动时  如何在手臂上绘制阴影  
as the arm is moving around in the scene.

166
00:07:36,435 --> 00:07:38,403
它当然不是完美的  
And it certainly isn't perfect.

167
00:07:38,403 --> 00:07:41,805
在周围仍然有些奇怪的颜色斑点  
There are still these little odd splotches of color around,

168
00:07:41,805 --> 00:07:43,808
但它肯定是在学习一些有关如何让图像看起来更真实的东西  
but it's definitely learning something

169
00:07:43,808 --> 00:07:46,956
但它肯定是在学习一些有关如何让图像看起来更真实的东西  
about what it means for an image to look more realistic.

170
00:07:47,028 --> 00:07:51,147
这对于获得大量漂亮的图像是很好的  
Now, this is good for getting a lot of pretty images,

171
00:07:51,147 --> 00:07:54,813
但是对于我们的问题而言  重要的是这些图像是否真的有用于
but what matters for our problem is whether these images are actually useful

172
00:07:54,813 --> 00:07:58,300
将它们减少到所需的实际数据  
for reducing them onto real-world data required.

173
00:07:58,960 --> 00:08:01,417
我们发现它的确可以  
And we find that it does.

174
00:08:01,417 --> 00:08:03,386
因此  稍微解释一下这个图表：
So, to explain this chart a bit:

175
00:08:03,386 --> 00:08:07,136
在x轴上是所使用的实际样本的数量  
On the x-axis is the number of real-world samples used,

176
00:08:07,136 --> 00:08:09,646
并且我们比较了不同方法的性能  
and we compared the performance of different methods

177
00:08:09,646 --> 00:08:13,057
在我们将它们改变为给模型提供的实际数据的同时我们这么做了  
as we vary them onto real-world data given to the model.

178
00:08:13,057 --> 00:08:16,727
当我们只使用模拟数据时  蓝条是我们的表现  
The blue bar is our performance when we use only simulated data.

179
00:08:16,727 --> 00:08:20,129
红色条是我们使用真实数据时的表现  
The red bar is our performance when we use only real data,

180
00:08:20,129 --> 00:08:24,297
橙色条是我们使用模拟和真实数据
and the orange bar is our performance when we use both simulated and real data

181
00:08:24,297 --> 00:08:27,667
以及我一直在谈论的领域适应方法时的表现  
and the domain adaptation methods that I've been talking about.

182
00:08:27,667 --> 00:08:30,611
我们发现当我们仅使用原始真实世界数据集的2％时  
And what we found is that when we use just 2%

183
00:08:30,611 --> 00:08:32,732
我们发现当我们仅使用原始真实世界数据集的2％时  
of our original real-world dataset,

184
00:08:32,801 --> 00:08:35,468
并且我们将领域适应应用于它  
and we apply domain adaptation to it,

185
00:08:35,470 --> 00:08:37,696
我们能够获得相同的性能水平  
we're able to get the same level of performance

186
00:08:37,696 --> 00:08:41,238
因此这减少了真实世界所需的样本数量
so this reduces the number of real-world samples we needed

187
00:08:41,238 --> 00:08:43,366
减少了50次  
by up to 50 times, which is really exciting

188
00:08:43,366 --> 00:08:46,515
这对于不需要大量时间运作机器人来学习这些抓握行为而言非常令人兴奋  
in terms of not needing to run robots for a large amount of time

189
00:08:46,515 --> 00:08:48,403
这对于不需要大量时间运作机器人来学习这些抓握行为而言非常令人兴奋  
to learn these grasping behaviors.

190
00:08:48,749 --> 00:08:52,149
此外  我们发现  即使我们将
Additionally, we found that even when we give

191
00:08:52,149 --> 00:08:54,652
所有真实世界的数据都提供给模型  
all of the real-world data to the model,

192
00:08:54,652 --> 00:08:56,586
当我们也提供模拟数据时  
when we give simulated data as well,

193
00:08:56,586 --> 00:08:59,055
我们仍然能够看到性能的进步  
we're still able to see improved performance

194
00:08:59,055 --> 00:09:01,589
这意味着我们在这个抓握问题上尚未达到数据容量限制  
so that implies that we haven't hit the data capacity limits

195
00:09:01,589 --> 00:09:03,316
这意味着我们在这个抓握问题上尚未达到数据容量限制  
for this grasping problem.

196
00:09:03,824 --> 00:09:07,561
最后  有一种方法可以在没有真实世界标签的情况下训练此设置  
And finally, there's a way to train this setup

197
00:09:07,561 --> 00:09:09,709
最后  有一种方法可以在没有真实世界标签的情况下训练此设置  
without having real-world labels,

198
00:09:09,859 --> 00:09:11,762
当我们在此环境中训练模型时  
and when we trained the model in this setting,

199
00:09:11,762 --> 00:09:14,665
我们发现我们仍然能够在现实世界的机器人上获得相当不错的性能  
we found that we were still able to get pretty good performance

200
00:09:14,665 --> 00:09:16,894
我们发现我们仍然能够在现实世界的机器人上获得相当不错的性能  
on the real-world robot.

201
00:09:19,091 --> 00:09:21,536
现在  这是 X 和 Brain 大型团队的工作成果  
Now, this was the work of a large team

202
00:09:21,536 --> 00:09:23,971
现在  这是 X 和 Brain 大型团队的工作成果  
across both Brain as well as X.

203
00:09:24,170 --> 00:09:26,406
我要感谢与我合作的所有人  
I'd like to thank all of my collaborators.

204
00:09:26,406 --> 00:09:28,641
这是原始文件的链接  
Here's a link to the original paper.

205
00:09:28,641 --> 00:09:31,341
如果人们有兴趣了解更多的细节  可以看看所提供的一篇博文  
And I believe there is also a blog post,

206
00:09:31,341 --> 00:09:34,262
如果人们有兴趣了解更多的细节  可以看看所提供的一篇博文  
if people are interested in hearing more details.

207
00:09:34,565 --> 00:09:35,675
谢谢  
Thanks.

