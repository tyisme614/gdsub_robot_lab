1
00:00:07,078 --> 00:00:09,306
大家好  
Alright, hi, everybody.

2
00:00:09,326 --> 00:00:12,208
我是来自 Brain Robotics 团队的 Alex  
I'm Alex, from the Brain Robotics team,

3
00:00:12,228 --> 00:00:15,777
在本演示中  我将谈论我们如何在一些
and in this presentation, I'll be talking about how we use simulation

4
00:00:15,797 --> 00:00:19,711
现实世界中的机器人学习问题中使用模拟和域适应  
and domain adaptation in some of our real-world robot learning problems.

5
00:00:20,631 --> 00:00:24,447
所以  首先让我开始介绍机器人学习  
So, first let me start by introducing robot learning.

6
00:00:24,467 --> 00:00:28,026
机器人学习的目标是使用机器学习
The goal of robot learning is to use machine learning

7
00:00:28,046 --> 00:00:29,816
来学习在一般环境中能运作的机器人技能  
to learn robotic skills

8
00:00:29,836 --> 00:00:31,780
来学习在一般环境中能运作的机器人技能  
that work in general environments.

9
00:00:32,005 --> 00:00:33,911
到目前为止  我们看到的是  
What we've seen so far is that

10
00:00:33,931 --> 00:00:35,721
如果你常常控制你的环境  
if you control your environment a lot,

11
00:00:35,741 --> 00:00:38,624
你可以让机器人做出很厉害的事情  
you can get robots to do pretty impressive things,

12
00:00:38,644 --> 00:00:40,158
而技术开始崩溃的地方
and where techniques start to break down

13
00:00:40,178 --> 00:00:42,782
是当你尝试将这些相同的技术应用
is when you try to apply these same techniques

14
00:00:42,802 --> 00:00:44,641
到更一般的环境时  
to more general environments.

15
00:00:44,661 --> 00:00:47,204
他们的思想是  如果你使用机器学习  
And the thinking is that if you use machine learning,

16
00:00:47,224 --> 00:00:48,924
那么你可以从你的环境中学习  
then you can learn from your environment,

17
00:00:48,944 --> 00:00:52,330
这可以帮助你解决这些泛化问题  
and this can help you address these generalization issues.

18
00:00:52,350 --> 00:00:54,148
所以  作为朝这个方向迈出的一步  
So, as a step in this direction,

19
00:00:54,168 --> 00:00:57,269
我们一直在关注机器人抓取的问题  
we've been looking at the problem of robotic grasping.

20
00:00:57,289 --> 00:00:59,272
这是我们一直在与X的一些人合作的一个项目  
This is a project that we've been working on

21
00:00:59,292 --> 00:01:01,508
这是我们一直在与X的一些人合作的一个项目  
in collaboration with some people at X.

22
00:01:01,793 --> 00:01:03,774
为了解释我们的问题设置  
And to explain our problem setup a bit,

23
00:01:03,794 --> 00:01:05,541
我们将有一个真正的机器人手臂  
we're going to have a real robot arm

24
00:01:05,561 --> 00:01:08,612
它正在学习从垃圾箱中拾取物体  
which is learning to pick up objects out of a bin.

25
00:01:08,632 --> 00:01:11,645
将会有一台摄像机俯视
There is going to be a camera looking down

26
00:01:11,665 --> 00:01:14,315
手臂的肩膀进入垃圾箱  
over the shoulder of the arm into the bin,

27
00:01:14,335 --> 00:01:17,750
从这张 RGB 图像中  我们将训练一个神经网络  
and from this RGB image we're going to train a neural network

28
00:01:17,770 --> 00:01:20,548
以了解它应该发送给机器人的命令
to learn what commands it should send to the robot

29
00:01:20,568 --> 00:01:22,914
以成功拾取物体  
to successfully pick up objects.

30
00:01:22,934 --> 00:01:27,255
现在  我们想尽量使用尽可能少的假设来解决这个任务  
Now, we want to try to solve this task using as few assumptions as possible.

31
00:01:27,275 --> 00:01:30,189
所以  重要的是  我们不会提供
So, importantly, we're not going to give any information

32
00:01:30,209 --> 00:01:33,489
关于我们想要拾取什么类型的物体的任何信息  
about the geometry of what kinds of objects we are trying to pick up,

33
00:01:33,509 --> 00:01:35,038
我们也不会提供
and we're also not going to give

34
00:01:35,058 --> 00:01:38,229
有关场景深度的任何信息  
any information about the depth of the scene.

35
00:01:38,249 --> 00:01:40,363
所以为了解决这个任务  
So in order to solve the task,

36
00:01:40,383 --> 00:01:42,567
模型需要学习手眼协调  
the model needs to learn hand-eye coordination

37
00:01:42,587 --> 00:01:45,202
或者需要知道它在摄像机图像中的位置  
or it needs to see where it is within the camera image,

38
00:01:45,222 --> 00:01:48,005
然后找出它在场景中的位置  
and then figure out where in the scene it is,

39
00:01:48,025 --> 00:01:50,719
然后结合这两者来弄清楚它应该如何移动  
and then combine these two to figure out how it should move around.

40
00:01:51,879 --> 00:01:55,495
现在  为了训练这个模型  我们将需要大量的数据  
Now, in order to train this model, we're going to need a lot of data

41
00:01:55,515 --> 00:01:57,656
因为它是一个相当大规模的图像模型  
because it's a pretty large scale image model.

42
00:01:57,676 --> 00:02:02,812
而我们当时的解决方案是使用更多的机器人  
And our solution at the time for this was to simply use more robots.

43
00:02:02,832 --> 00:02:05,551
所以这就是我们所说的  手臂农场    
So this is what we call the "Arm Farm."

44
00:02:05,571 --> 00:02:08,681
有六台机器人并行采集数据  
These are six robots collecting data in parallel.

45
00:02:08,701 --> 00:02:11,544
如果你有六个机器人  你可以更快地收集数据  
And if you have six robots, you can collect data a lot faster

46
00:02:11,564 --> 00:02:13,251
如果跟只有一个机器人相比  
than if you only have one robot.

47
00:02:13,271 --> 00:02:16,221
因此  使用这些机器人  我们能够在数千个机器人小时内收集超过一百万次抓握的尝试  
So using these robots, we were able to collect

48
00:02:16,241 --> 00:02:18,687
因此  使用这些机器人  我们能够在数千个机器人小时内收集超过一百万次抓握的尝试  
over a million attempted grasps,

49
00:02:18,707 --> 00:02:20,708
因此  使用这些机器人  我们能够在数千个机器人小时内收集超过一百万次抓握的尝试  
over a total of thousands of robot hours,

50
00:02:20,933 --> 00:02:22,661
然后使用这些数据  我们能够
and then using this data we were able

51
00:02:22,681 --> 00:02:26,592
成功地训练模型学习如何拾取物体  
to successfully train models to learn how to pick up objects.

52
00:02:26,612 --> 00:02:29,396
这是可行  但仍需要很长时间
Now, this works, but it still took a lot of time

53
00:02:29,416 --> 00:02:31,030
才能收集此数据集  
to collect this dataset.

54
00:02:31,050 --> 00:02:33,031
因此  这一点激励我们寻找方法
So this motivated looking into ways

55
00:02:33,051 --> 00:02:35,089
减少学习这些行为所需的实际数据量  
to reduce the amount of real-world data needed

56
00:02:35,109 --> 00:02:38,848
减少学习这些行为所需的实际数据量  
to learn these behaviors.

57
00:02:38,996 --> 00:02:41,646
一种做法是模拟  
One approach for doing this is simulation.

58
00:02:41,666 --> 00:02:43,304
因此  在左侧视频中  
So in the left video here,

59
00:02:43,324 --> 00:02:45,851
你可以看到在我们的真实世界设置中进入我们模型的图像  
you can see the images that are going into our model

60
00:02:45,871 --> 00:02:47,524
你可以看到在我们的真实世界设置中进入我们模型的图像  
in our real world setup,

61
00:02:47,793 --> 00:02:49,775
而在右侧  你可以看到
and on the right here you can see

62
00:02:49,795 --> 00:02:52,690
我们模拟该设置的重新创建  
our simulated recreation of that setup.

63
00:02:52,710 --> 00:02:55,212
现在  将事物转化为模拟的优势在于
Now, the advantage of moving things into simulation

64
00:02:55,232 --> 00:02:58,316
模拟机器人更容易扩展  
is that simulated robots are a lot easier to scale.

65
00:02:58,336 --> 00:03:01,450
我们已经能够旋转数以千计的模拟机器人  
We've been able to spin up thousands of simulated robots

66
00:03:01,470 --> 00:03:03,318
它们会抓住各种物体
grasping various objects,

67
00:03:03,338 --> 00:03:06,426
使用这种设置  我们能够在短短八个多小时内收集数百万的抓握  
and using this setup we were able to collect millions of grasps

68
00:03:06,446 --> 00:03:07,900
使用这种设置  我们能够在短短八个多小时内收集数百万的抓握  
in just over eight hours,

69
00:03:08,137 --> 00:03:10,998
而不是原始数据集所需的几周  
instead of the weeks that were required for our original dataset.

70
00:03:12,846 --> 00:03:15,921
这对于获取大量数据非常有用  
Now, this is good for getting a lot of data,

71
00:03:15,941 --> 00:03:18,525
但不幸的是  模拟培训的模型
but unfortunately models trained in simulation

72
00:03:18,545 --> 00:03:21,741
往往不会转移到真实世界的机器人  
tend not to transfer to the actual real world robot.

73
00:03:21,761 --> 00:03:24,697
两者之间存在很多系统性差异  
There are a lot of systematic differences between the two.

74
00:03:24,717 --> 00:03:28,010
其中一件大事是不同事物的视觉外观  
One big one is the visual appearances of different things.

75
00:03:28,030 --> 00:03:30,336
而另一个重要的就是我们现实世界的物理学和我们的模拟物理学之间的物理差异  
And another big one is just physical differences

76
00:03:30,356 --> 00:03:31,933
而另一个重要的就是我们现实世界的物理学和我们的模拟物理学之间的物理差异  
between our real-world physics

77
00:03:31,953 --> 00:03:34,050
而另一个重要的就是我们现实世界的物理学和我们的模拟物理学之间的物理差异  
and our simulated physics.

78
00:03:34,189 --> 00:03:37,171
所以我们所做的就是  我们能够快速
So what we did was, we were able to very quickly

79
00:03:37,191 --> 00:03:41,373
训练我们的模拟模型  以获得大约90％的抓握成功  
train our model on simulation to get to around 90% grasp success.

80
00:03:41,393 --> 00:03:43,639
然后  我们部署到真正的机器人上  
We then deployed to the real robot,

81
00:03:43,659 --> 00:03:46,387
它的成功率超过20％  
and it succeeds just over 20% of the time,

82
00:03:46,407 --> 00:03:48,409
这是一个非常大的性能下降  
which is a very big performance drop.

83
00:03:49,049 --> 00:03:51,510
所以为了获得好的表现  
So in order to actually get good performance,

84
00:03:51,530 --> 00:03:53,831
我们的做法需要更聪明一些  
we need to do something a bit more clever.

85
00:03:53,851 --> 00:03:56,680
所以这个动机考虑了模拟 - 到-现实的转换  
So this motivated looking into Sim-to-Real transfer,

86
00:03:56,700 --> 00:03:59,353
这是一组转换学习技术  
which is a set of transfer-learning techniques

87
00:03:59,373 --> 00:04:01,254
用于尝试使用模拟数据
for trying to use simulated data

88
00:04:01,274 --> 00:04:04,489
来提高你的实际样本效率  
to improve your real-world sample efficiency.

89
00:04:06,559 --> 00:04:09,959
有几种不同的方法可以做到这一点  
Now, there are a few different ways you can do this.

90
00:04:09,979 --> 00:04:11,624
这样做的一种方法是在
One approach for doing this is

91
00:04:11,644 --> 00:04:14,025
你的模拟器中增加更多的随机化  
adding more randomization into your simulator.

92
00:04:14,045 --> 00:04:16,459
你可以通过更改应用于不同物体的纹理  
You can do this by changing around the textures

93
00:04:16,479 --> 00:04:17,919
你可以通过更改应用于不同物体的纹理  
that you apply to different objects,

94
00:04:17,989 --> 00:04:19,943
改变其颜色  
changing around their colors,

95
00:04:19,963 --> 00:04:22,935
改变灯光与场景的交互方式  
changing how lighting is interacting with your scene,

96
00:04:22,955 --> 00:04:27,103
以及改变所要尝试捡起的物体的几何形状来实现这一点  
and you can also play around with changing the geometry of what kinds of objects

97
00:04:27,123 --> 00:04:28,344
以及改变所要尝试捡起的物体的几何形状来实现这一点  
you're trying to pick up.

98
00:04:28,575 --> 00:04:31,936
另一种方法是领域适应  
Another way of doing this is domain adaptation,

99
00:04:31,956 --> 00:04:34,242
这是一组学习技术  
which is a set of techniques for learning

100
00:04:34,262 --> 00:04:38,111
这适用于当你有一些共同的结构的两个数据领域  
when you have two domains of data that have some common structure,

101
00:04:38,131 --> 00:04:39,847
但仍然有些不同  
but are still somewhat different.

102
00:04:39,867 --> 00:04:42,720
在我们的情况下  这两个领域将是我们的模拟机器人数据
In our case the two domains are going to be our simulated robot data

103
00:04:42,740 --> 00:04:45,293
和我们的真实机器人数据  
and our real robot data.

104
00:04:45,313 --> 00:04:47,446
还有一些功能级别的方法可以做到这一点  
And there are feature-level ways of doing this

105
00:04:47,466 --> 00:04:49,815
并且也有像素级别的方法可以做到这一点  
and there are pixel-level ways of doing this.

106
00:04:49,835 --> 00:04:53,124
现在  在这项工作中  我们尝试了所有这些方法  
Now, in this work, we tried all of these approaches,

107
00:04:53,144 --> 00:04:55,668
并且在本演示中  我将首先关注
and in this presentation, I'm going to focus primarily

108
00:04:55,688 --> 00:04:58,103
领域适应这方面  
on the domain adaptation side of things.

109
00:05:00,513 --> 00:05:02,927
因此  在特征级别领域培训中  
So, in feature-level domain adaptation

110
00:05:02,947 --> 00:05:06,427
我们要做的是我们将采用我们的模拟数据  
what we're going to do is we're going to take our simulated data,

111
00:05:06,447 --> 00:05:07,794
拿出我们的真实数据  
take our real data,

112
00:05:07,814 --> 00:05:10,660
在两个数据集上训练相同的模型  
train the same model on both datasets,

113
00:05:10,680 --> 00:05:13,098
然后在网络的中间特征层  
but then at an intermediate feature layer of the network,

114
00:05:13,118 --> 00:05:15,637
我们将附上相似性损失  
we're going to attach a similarity loss.

115
00:05:15,657 --> 00:05:19,473
而相似性损失将会促使特征的分布
And the similarity loss is going to encourage the distribution of features

116
00:05:19,493 --> 00:05:21,520
在整个领域变得相同  
to be the same across both domains.

117
00:05:21,540 --> 00:05:24,928
一种很好的方法
Now, one approach for doing this which has worked well recently

118
00:05:24,948 --> 00:05:27,745
被称之为领域 - 对抗神经网络  
is called Domain-Adversarial Neural Networks.

119
00:05:27,765 --> 00:05:31,412
它的运作的方式是将相似性损失
And the way these work is that the similarity loss is implemented

120
00:05:31,432 --> 00:05:34,715
作为一个小型的神经网络来实现  它试图根据它所接收到的输入特征来预测领域  
as a small neural net that tries to predict the domain

121
00:05:34,735 --> 00:05:37,230
作为一个小型的神经网络来实现  它试图根据它所接收到的输入特征来预测领域  
based on the input features it's receiving,

122
00:05:37,420 --> 00:05:39,113
然后其余的模型试图尽可能地
and then the rest of the model is trying

123
00:05:39,133 --> 00:05:42,233
混淆这个领域分类器  
to confuse this domain classifier as much as possible.

124
00:05:44,683 --> 00:05:48,056
像素级别的方法尝试从不同的角度解决问题  
Now, pixel-level methods try to work at the problem

125
00:05:48,076 --> 00:05:49,542
像素级别的方法尝试从不同的角度解决问题  
from a different point of view.

126
00:05:49,711 --> 00:05:52,693
我们尝试让像素级别的图像看起来更逼真  而不是试图学习领域不变特征
Instead of trying to learn domain invariant features,

127
00:05:52,713 --> 00:05:55,757
我们尝试让像素级别的图像看起来更逼真  而不是试图学习领域不变特征
we're going to try to transform our images at the pixel level

128
00:05:55,777 --> 00:05:57,326
我们尝试让像素级别的图像看起来更逼真  而不是试图学习领域不变特征
to look more realistic.

129
00:05:57,517 --> 00:06:01,063
所以我们在这里的做法是我们采取一个生成对抗网络;
So what we do here is we take a generative-adversarial network;

130
00:06:01,083 --> 00:06:04,767
我们从模拟器中提供一张图像  
we feed it an image from our simulator,

131
00:06:04,787 --> 00:06:08,568
然后输出看起来更逼真的图像  
and then it's going to output an image that looks more realistic.

132
00:06:08,588 --> 00:06:11,504
然后我们将使用这个生成器的输出
And then we're going to use the output of this generator

133
00:06:11,524 --> 00:06:15,401
来训练我们想要训练的任何任务模型  
to train whatever task model that we want to train.

134
00:06:15,421 --> 00:06:16,942
现在我们要同时训练
Now we're going to train both

135
00:06:16,962 --> 00:06:19,344
发电机和任务模型  
the generator and the task model at the same time.

136
00:06:19,364 --> 00:06:21,377
我们发现在实践中  这很有用  
We found that in practice, this was useful

137
00:06:21,397 --> 00:06:23,811
因为它有助于将发电机输出信号
because it helps ground the generator output

138
00:06:23,831 --> 00:06:27,072
用于实际训练下游任务  
to be useful for actually training your downstream task.

139
00:06:29,242 --> 00:06:31,917
好的  退一步说  
Alright. So taking a step back,

140
00:06:31,937 --> 00:06:35,991
功能级别的方法可以在你有不完全相同的相关领域的数据时学习域不变特征  
feature-level methods can learn domain-invariant features

141
00:06:36,011 --> 00:06:38,090
功能级别的方法可以在你有不完全相同的相关领域的数据时学习域不变特征  
when you have data from related domains

142
00:06:38,110 --> 00:06:39,980
功能级别的方法可以在你有不完全相同的相关领域的数据时学习域不变特征  
that aren't quite identical.

143
00:06:40,279 --> 00:06:43,592
同时  像素级方法可以将数据
Meanwhile, pixel-level methods can transform your data

144
00:06:43,612 --> 00:06:46,393
转换为更接近真实世界的数据  
to look more like your real-world data,

145
00:06:46,413 --> 00:06:48,128
但实际上它们的运作不是完美的  
but in practice they don't work perfectly,

146
00:06:48,148 --> 00:06:50,195
并且发生器输出中仍然存在一些小的瑕疵和不准确性  
and there are still some small artifacts

147
00:06:50,215 --> 00:06:52,327
并且发生器输出中仍然存在一些小的瑕疵和不准确性  
and inaccuracies from the generator output.

148
00:06:52,753 --> 00:06:56,702
所以我们的想法是    为什么我们不把这两种方法结合起来    
So our thinking went, "Why don't we simply combine both of these approaches?"

149
00:06:56,722 --> 00:06:59,181
我们可以应用像素级别的方法
We can apply a pixel-level method

150
00:06:59,201 --> 00:07:01,903
尽可能多地转换数据  
to try to transform the data as much as possible,

151
00:07:01,923 --> 00:07:04,240
但这不会让我们一路畅通无阻  
and this isn't going to get us all the way there,

152
00:07:04,260 --> 00:07:06,941
但是随后可以在此基础上附加一个功能级别的方法  
but then we can attach a feature-level method on top of this

153
00:07:06,961 --> 00:07:09,905
以尝试进一步缩小现实的差距  
to try to close the reality gap even further,

154
00:07:09,925 --> 00:07:12,498
并将这些形式组合在一起  形成我们所说的抓取 gen  
and combined these form what we call the grasp gen

155
00:07:12,518 --> 00:07:13,956
它是像素级和特征级域适应的组合  
which is a combination of both

156
00:07:13,976 --> 00:07:16,134
它是像素级和特征级域适应的组合  
pixel-level and feature-level domain adaptation.

157
00:07:16,232 --> 00:07:18,247
在这视频的左半部分  
In the left half of the video here

158
00:07:18,267 --> 00:07:19,847
你可以看到一个模拟的抓握  
you can see a simulated grasp.

159
00:07:19,867 --> 00:07:22,851
在右半边你可以看到我们的发电机的输出  
In the right half you can see the output of our generator.

160
00:07:22,871 --> 00:07:25,218
你可以看到它正在学习一些非常酷的东西  
And you can see that it's learning some pretty cool things

161
00:07:25,238 --> 00:07:27,720
从绘制托盘应该看起来的样子  
in terms of drawing what the tray should look like,

162
00:07:27,740 --> 00:07:30,323
在手臂上绘制更逼真的纹理  
drawing more realistic textures on the arm,

163
00:07:30,343 --> 00:07:32,225
绘制物体正在投射的阴影  
drawing shadows that the objects are casting.

164
00:07:32,245 --> 00:07:34,367
它还学会了当手臂在场景中移动时  如何在手臂上绘制阴影  
It's also learned how to even draw shadows

165
00:07:34,387 --> 00:07:36,354
它还学会了当手臂在场景中移动时  如何在手臂上绘制阴影  
as the arm is moving around in the scene.

166
00:07:36,445 --> 00:07:38,393
它当然不是完美的  
And it certainly isn't perfect.

167
00:07:38,413 --> 00:07:41,795
在周围仍然有些奇怪的颜色斑点  
There are still these little odd splotches of color around,

168
00:07:41,815 --> 00:07:43,798
但它肯定是在学习一些有关如何让图像看起来更真实的东西  
but it's definitely learning something

169
00:07:43,818 --> 00:07:46,946
但它肯定是在学习一些有关如何让图像看起来更真实的东西  
about what it means for an image to look more realistic.

170
00:07:47,038 --> 00:07:51,137
这对于获得大量漂亮的图像是很好的  
Now, this is good for getting a lot of pretty images,

171
00:07:51,157 --> 00:07:54,803
但是对于我们的问题而言  重要的是这些图像是否真的有用于
but what matters for our problem is whether these images are actually useful

172
00:07:54,823 --> 00:07:58,290
将它们减少到所需的实际数据  
for reducing them onto real-world data required.

173
00:07:58,970 --> 00:08:01,407
我们发现它的确可以  
And we find that it does.

174
00:08:01,427 --> 00:08:03,376
因此  稍微解释一下这个图表：
So, to explain this chart a bit:

175
00:08:03,396 --> 00:08:07,126
在x轴上是所使用的实际样本的数量  
On the x-axis is the number of real-world samples used,

176
00:08:07,146 --> 00:08:09,636
并且我们比较了不同方法的性能  
and we compared the performance of different methods

177
00:08:09,656 --> 00:08:13,047
在我们将它们改变为给模型提供的实际数据的同时我们这么做了  
as we vary them onto real-world data given to the model.

178
00:08:13,067 --> 00:08:16,717
当我们只使用模拟数据时  蓝条是我们的表现  
The blue bar is our performance when we use only simulated data.

179
00:08:16,737 --> 00:08:20,119
红色条是我们使用真实数据时的表现  
The red bar is our performance when we use only real data,

180
00:08:20,139 --> 00:08:24,287
橙色条是我们使用模拟和真实数据
and the orange bar is our performance when we use both simulated and real data

181
00:08:24,307 --> 00:08:27,657
以及我一直在谈论的领域适应方法时的表现  
and the domain adaptation methods that I've been talking about.

182
00:08:27,677 --> 00:08:30,601
我们发现当我们仅使用原始真实世界数据集的2％时  
And what we found is that when we use just 2%

183
00:08:30,621 --> 00:08:32,722
我们发现当我们仅使用原始真实世界数据集的2％时  
of our original real-world dataset,

184
00:08:32,811 --> 00:08:35,458
并且我们将领域适应应用于它  
and we apply domain adaptation to it,

185
00:08:35,480 --> 00:08:37,686
我们能够获得相同的性能水平  
we're able to get the same level of performance

186
00:08:37,706 --> 00:08:41,228
因此这减少了真实世界所需的样本数量
so this reduces the number of real-world samples we needed

187
00:08:41,248 --> 00:08:43,356
减少了50次  
by up to 50 times, which is really exciting

188
00:08:43,376 --> 00:08:46,505
这对于不需要大量时间运作机器人来学习这些抓握行为而言非常令人兴奋  
in terms of not needing to run robots for a large amount of time

189
00:08:46,525 --> 00:08:48,393
这对于不需要大量时间运作机器人来学习这些抓握行为而言非常令人兴奋  
to learn these grasping behaviors.

190
00:08:48,759 --> 00:08:52,139
此外  我们发现  即使我们将
Additionally, we found that even when we give

191
00:08:52,159 --> 00:08:54,642
所有真实世界的数据都提供给模型  
all of the real-world data to the model,

192
00:08:54,662 --> 00:08:56,576
当我们也提供模拟数据时  
when we give simulated data as well,

193
00:08:56,596 --> 00:08:59,045
我们仍然能够看到性能的进步  
we're still able to see improved performance

194
00:08:59,065 --> 00:09:01,579
这意味着我们在这个抓握问题上尚未达到数据容量限制  
so that implies that we haven't hit the data capacity limits

195
00:09:01,599 --> 00:09:03,306
这意味着我们在这个抓握问题上尚未达到数据容量限制  
for this grasping problem.

196
00:09:03,834 --> 00:09:07,551
最后  有一种方法可以在没有真实世界标签的情况下训练此设置  
And finally, there's a way to train this setup

197
00:09:07,571 --> 00:09:09,699
最后  有一种方法可以在没有真实世界标签的情况下训练此设置  
without having real-world labels,

198
00:09:09,869 --> 00:09:11,752
当我们在此环境中训练模型时  
and when we trained the model in this setting,

199
00:09:11,772 --> 00:09:14,655
我们发现我们仍然能够在现实世界的机器人上获得相当不错的性能  
we found that we were still able to get pretty good performance

200
00:09:14,675 --> 00:09:16,884
我们发现我们仍然能够在现实世界的机器人上获得相当不错的性能  
on the real-world robot.

201
00:09:19,101 --> 00:09:21,526
现在  这是 X 和 Brain 大型团队的工作成果  
Now, this was the work of a large team

202
00:09:21,546 --> 00:09:23,961
现在  这是 X 和 Brain 大型团队的工作成果  
across both Brain as well as X.

203
00:09:24,180 --> 00:09:26,396
我要感谢与我合作的所有人  
I'd like to thank all of my collaborators.

204
00:09:26,416 --> 00:09:28,631
这是原始文件的链接  
Here's a link to the original paper.

205
00:09:28,651 --> 00:09:31,331
如果人们有兴趣了解更多的细节  可以看看所提供的一篇博文  
And I believe there is also a blog post,

206
00:09:31,351 --> 00:09:34,252
如果人们有兴趣了解更多的细节  可以看看所提供的一篇博文  
if people are interested in hearing more details.

207
00:09:34,575 --> 00:09:35,665
谢谢  
Thanks.

