WEBVTT
Kind: captions
Language: zh-Hans

00:00:07.068 --> 00:00:09.316
大家好。

00:00:09.316 --> 00:00:12.218
我是来自 Brain Robotics 团队的 Alex，

00:00:12.218 --> 00:00:15.787
在本演示中，我将谈论我们如何在一些

00:00:15.787 --> 00:00:19.721
现实世界中的机器人学习问题中
使用模拟和域适应。

00:00:20.621 --> 00:00:24.457
所以，首先让我开始介绍机器人学习。

00:00:24.457 --> 00:00:28.036
机器人学习的目标是使用机器学习

00:00:28.036 --> 00:00:31.790
来学习在一般环境中能运作的机器人技能。

00:00:31.995 --> 00:00:33.921
到目前为止，我们看到的是，

00:00:33.921 --> 00:00:35.731
如果你常常控制你的环境，

00:00:35.731 --> 00:00:38.634
你可以让机器人做出很厉害的事情，

00:00:38.634 --> 00:00:40.168
而技术开始崩溃的地方

00:00:40.168 --> 00:00:42.792
是当你尝试将这些相同的技术应用

00:00:42.792 --> 00:00:44.651
到更一般的环境时。

00:00:44.651 --> 00:00:47.214
他们的思想是，如果你使用机器学习，

00:00:47.214 --> 00:00:48.934
那么你可以从你的环境中学习，

00:00:48.934 --> 00:00:52.340
这可以帮助你解决这些泛化问题。

00:00:52.340 --> 00:00:54.158
所以，作为朝这个方向迈出的一步，

00:00:54.158 --> 00:00:57.279
我们一直在关注机器人抓取的问题。

00:00:57.279 --> 00:01:01.518
这是我们一直在与X的一些人合作的一个项目。

00:01:01.783 --> 00:01:03.784
为了解释我们的问题设置，

00:01:03.784 --> 00:01:05.551
我们将有一个真正的机器人手臂，

00:01:05.551 --> 00:01:08.622
它正在学习从垃圾箱中拾取物体。

00:01:08.622 --> 00:01:11.655
将会有一台摄像机俯视

00:01:11.655 --> 00:01:14.325
手臂的肩膀进入垃圾箱，

00:01:14.325 --> 00:01:17.760
从这张 RGB 图像中，我们将训练一个神经网络，

00:01:17.760 --> 00:01:20.558
以了解它应该发送给机器人的命令

00:01:20.558 --> 00:01:22.924
以成功拾取物体。

00:01:22.924 --> 00:01:27.265
现在，我们想尽量使用尽可能少的假设来解决这个任务。

00:01:27.265 --> 00:01:30.199
所以，重要的是，我们不会提供

00:01:30.199 --> 00:01:33.499
关于我们想要拾取什么类型的物体的任何信息，

00:01:33.499 --> 00:01:35.048
我们也不会提供

00:01:35.048 --> 00:01:38.239
有关场景深度的任何信息。

00:01:38.239 --> 00:01:40.373
所以为了解决这个任务，

00:01:40.373 --> 00:01:42.577
模型需要学习手眼协调，

00:01:42.577 --> 00:01:45.212
或者需要知道它在摄像机图像中的位置，

00:01:45.212 --> 00:01:48.015
然后找出它在场景中的位置，

00:01:48.015 --> 00:01:50.729
然后结合这两者来弄清楚它应该如何移动。

00:01:51.869 --> 00:01:55.505
现在，为了训练这个模型，
我们将需要大量的数据，

00:01:55.505 --> 00:01:57.666
因为它是一个相当大规模的图像模型。

00:01:57.666 --> 00:02:02.822
而我们当时的解决方案是使用更多的机器人。

00:02:02.822 --> 00:02:05.561
所以这就是我们所说的“手臂农场”。

00:02:05.561 --> 00:02:08.691
有六台机器人并行采集数据。

00:02:08.691 --> 00:02:11.554
如果你有六个机器人，你可以更快地收集数据。

00:02:11.554 --> 00:02:13.261
如果跟只有一个机器人相比。

00:02:13.261 --> 00:02:20.718
因此，使用这些机器人，我们能够在数千个
机器人小时内收集超过一百万次抓握的尝试，

00:02:20.923 --> 00:02:22.671
然后使用这些数据，我们能够

00:02:22.671 --> 00:02:26.602
成功地训练模型学习如何拾取物体。

00:02:26.602 --> 00:02:29.406
这是可行，但仍需要很长时间

00:02:29.406 --> 00:02:31.040
才能收集此数据集。

00:02:31.040 --> 00:02:33.041
因此，这一点激励我们寻找方法

00:02:33.041 --> 00:02:38.858
减少学习这些行为所需的实际数据量。

00:02:38.986 --> 00:02:41.656
一种做法是模拟。

00:02:41.656 --> 00:02:43.314
因此，在左侧视频中，

00:02:43.314 --> 00:02:47.534
你可以看到在我们的真实世界设置中
进入我们模型的图像，

00:02:47.783 --> 00:02:49.785
而在右侧，你可以看到

00:02:49.785 --> 00:02:52.700
我们模拟该设置的重新创建。

00:02:52.700 --> 00:02:55.222
现在，将事物转化为模拟的优势在于

00:02:55.222 --> 00:02:58.326
模拟机器人更容易扩展。

00:02:58.326 --> 00:03:01.460
我们已经能够旋转数以千计的模拟机器人，

00:03:01.460 --> 00:03:03.328
它们会抓住各种物体

00:03:03.328 --> 00:03:07.910
使用这种设置，我们能够在短短八个多小时内
收集数百万的抓握，

00:03:08.127 --> 00:03:11.008
而不是原始数据集所需的几周。

00:03:12.836 --> 00:03:15.931
这对于获取大量数据非常有用，

00:03:15.931 --> 00:03:18.535
但不幸的是，模拟培训的模型

00:03:18.535 --> 00:03:21.751
往往不会转移到真实世界的机器人。

00:03:21.751 --> 00:03:24.707
两者之间存在很多系统性差异。

00:03:24.707 --> 00:03:28.020
其中一件大事是不同事物的视觉外观。

00:03:28.020 --> 00:03:34.060
而另一个重要的就是我们现实世界的物理学和
我们的模拟物理学之间的物理差异。

00:03:34.179 --> 00:03:37.181
所以我们所做的就是，我们能够快速

00:03:37.181 --> 00:03:41.383
训练我们的模拟模型，以获得大约90％的抓握成功。

00:03:41.383 --> 00:03:43.649
然后，我们部署到真正的机器人上，

00:03:43.649 --> 00:03:46.397
它的成功率超过20％，

00:03:46.397 --> 00:03:48.419
这是一个非常大的性能下降。

00:03:49.039 --> 00:03:51.520
所以为了获得好的表现，

00:03:51.520 --> 00:03:53.841
我们的做法需要更聪明一些。

00:03:53.841 --> 00:03:56.690
所以这个动机考虑了模拟 - 到-现实的转换，

00:03:56.690 --> 00:03:59.363
这是一组转换学习技术，

00:03:59.363 --> 00:04:01.264
用于尝试使用模拟数据

00:04:01.264 --> 00:04:04.499
来提高你的实际样本效率。

00:04:06.549 --> 00:04:09.969
有几种不同的方法可以做到这一点。

00:04:09.969 --> 00:04:11.634
这样做的一种方法是在

00:04:11.634 --> 00:04:14.035
你的模拟器中增加更多的随机化。

00:04:14.035 --> 00:04:17.929
你可以通过更改应用于不同物体的纹理，

00:04:17.979 --> 00:04:19.953
改变其颜色，

00:04:19.953 --> 00:04:22.945
改变灯光与场景的交互方式，

00:04:22.945 --> 00:04:28.354
以及改变所要尝试捡起的物体的
几何形状来实现这一点。

00:04:28.565 --> 00:04:31.946
另一种方法是领域适应，

00:04:31.946 --> 00:04:34.252
这是一组学习技术，

00:04:34.252 --> 00:04:38.121
这适用于当你有一些共同的结构的两个数据领域，

00:04:38.121 --> 00:04:39.857
但仍然有些不同。

00:04:39.857 --> 00:04:42.730
在我们的情况下，这两个领域将是
我们的模拟机器人数据

00:04:42.730 --> 00:04:45.303
和我们的真实机器人数据。

00:04:45.303 --> 00:04:47.456
还有一些功能级别的方法可以做到这一点，

00:04:47.456 --> 00:04:49.825
并且也有像素级别的方法可以做到这一点。

00:04:49.825 --> 00:04:53.134
现在，在这项工作中，
我们尝试了所有这些方法，

00:04:53.134 --> 00:04:55.678
并且在本演示中，我将首先关注

00:04:55.678 --> 00:04:58.113
领域适应这方面。

00:05:00.503 --> 00:05:02.937
因此，在特征级别领域培训中，

00:05:02.937 --> 00:05:06.437
我们要做的是我们将采用我们的模拟数据，

00:05:06.437 --> 00:05:07.804
拿出我们的真实数据，

00:05:07.804 --> 00:05:10.670
在两个数据集上训练相同的模型，

00:05:10.670 --> 00:05:13.108
然后在网络的中间特征层，

00:05:13.108 --> 00:05:15.647
我们将附上相似性损失。

00:05:15.647 --> 00:05:19.483
而相似性损失将会促使特征的分布

00:05:19.483 --> 00:05:21.530
在整个领域变得相同。

00:05:21.530 --> 00:05:24.938
一种很好的方法

00:05:24.938 --> 00:05:27.755
被称之为领域 - 对抗神经网络。

00:05:27.755 --> 00:05:31.422
它的运作的方式是将相似性损失

00:05:31.422 --> 00:05:37.240
作为一个小型的神经网络来实现，
它试图根据它所接收到的输入特征来预测领域，

00:05:37.410 --> 00:05:39.123
然后其余的模型试图尽可能地

00:05:39.123 --> 00:05:42.243
混淆这个领域分类器。

00:05:44.673 --> 00:05:49.552
像素级别的方法尝试从不同的角度解决问题。

00:05:49.701 --> 00:05:57.336
我们尝试让像素级别的图像看起来更逼真，
而不是试图学习领域不变特征

00:05:57.507 --> 00:06:01.073
所以我们在这里的做法是我们采取一个生成对抗网络;

00:06:01.073 --> 00:06:04.777
我们从模拟器中提供一张图像，

00:06:04.777 --> 00:06:08.578
然后输出看起来更逼真的图像。

00:06:08.578 --> 00:06:11.514
然后我们将使用这个生成器的输出

00:06:11.514 --> 00:06:15.411
来训练我们想要训练的任何任务模型。

00:06:15.411 --> 00:06:16.952
现在我们要同时训练

00:06:16.952 --> 00:06:19.354
发电机和任务模型。

00:06:19.354 --> 00:06:21.387
我们发现在实践中，这很有用，

00:06:21.387 --> 00:06:23.821
因为它有助于将发电机输出信号

00:06:23.821 --> 00:06:27.082
用于实际训练下游任务。

00:06:29.232 --> 00:06:31.927
好的。退一步说，

00:06:31.927 --> 00:06:39.990
功能级别的方法可以在你有不完全相同的
相关领域的数据时学习域不变特征。

00:06:40.269 --> 00:06:43.602
同时，像素级方法可以将数据

00:06:43.602 --> 00:06:46.403
转换为更接近真实世界的数据，

00:06:46.403 --> 00:06:48.138
但实际上它们的运作不是完美的，

00:06:48.138 --> 00:06:52.337
并且发生器输出中仍然存在
一些小的瑕疵和不准确性。

00:06:52.743 --> 00:06:56.712
所以我们的想法是，
“为什么我们不把这两种方法结合起来？”

00:06:56.712 --> 00:06:59.191
我们可以应用像素级别的方法

00:06:59.191 --> 00:07:01.913
尽可能多地转换数据，

00:07:01.913 --> 00:07:04.250
但这不会让我们一路畅通无阻，

00:07:04.250 --> 00:07:06.951
但是随后可以在此基础上附加
一个功能级别的方法，

00:07:06.951 --> 00:07:09.915
以尝试进一步缩小现实的差距，

00:07:09.915 --> 00:07:12.508
并将这些形式组合在一起，
形成我们所说的抓取 gen，

00:07:12.508 --> 00:07:16.144
它是像素级和特征级域适应的组合。

00:07:16.222 --> 00:07:18.257
在这视频的左半部分，

00:07:18.257 --> 00:07:19.857
你可以看到一个模拟的抓握。

00:07:19.857 --> 00:07:22.861
在右半边你可以看到我们的发电机的输出。

00:07:22.861 --> 00:07:25.228
你可以看到它正在学习一些非常酷的东西，

00:07:25.228 --> 00:07:27.730
从绘制托盘应该看起来的样子，

00:07:27.730 --> 00:07:30.333
在手臂上绘制更逼真的纹理，

00:07:30.333 --> 00:07:32.235
绘制物体正在投射的阴影。

00:07:32.235 --> 00:07:36.364
它还学会了当手臂在场景中移动时，
如何在手臂上绘制阴影。

00:07:36.435 --> 00:07:38.403
它当然不是完美的。

00:07:38.403 --> 00:07:41.805
在周围仍然有些奇怪的颜色斑点，

00:07:41.805 --> 00:07:46.956
但它肯定是在学习一些有关
如何让图像看起来更真实的东西。

00:07:47.028 --> 00:07:51.147
这对于获得大量漂亮的图像是很好的，

00:07:51.147 --> 00:07:54.813
但是对于我们的问题而言，
重要的是这些图像是否真的有用于

00:07:54.813 --> 00:07:58.300
将它们减少到所需的实际数据。

00:07:58.960 --> 00:08:01.417
我们发现它的确可以。

00:08:01.417 --> 00:08:03.386
因此，稍微解释一下这个图表：

00:08:03.386 --> 00:08:07.136
在x轴上是所使用的实际样本的数量，

00:08:07.136 --> 00:08:09.646
并且我们比较了不同方法的性能，

00:08:09.646 --> 00:08:13.057
在我们将它们改变为给模型提供的
实际数据的同时我们这么做了。

00:08:13.057 --> 00:08:16.727
当我们只使用模拟数据时，蓝条是我们的表现。

00:08:16.727 --> 00:08:20.129
红色条是我们使用真实数据时的表现，

00:08:20.129 --> 00:08:24.297
橙色条是我们使用模拟和真实数据

00:08:24.297 --> 00:08:27.667
以及我一直在谈论的领域适应方法时的表现。

00:08:27.667 --> 00:08:32.732
我们发现当我们仅使用
原始真实世界数据集的2％时，

00:08:32.801 --> 00:08:35.468
并且我们将领域适应应用于它，

00:08:35.470 --> 00:08:37.696
我们能够获得相同的性能水平，

00:08:37.696 --> 00:08:41.238
因此这减少了真实世界所需的样本数量

00:08:41.238 --> 00:08:43.366
减少了50次，

00:08:43.366 --> 00:08:48.403
这对于不需要大量时间运作机器人
来学习这些抓握行为而言非常令人兴奋。

00:08:48.749 --> 00:08:52.149
此外，我们发现，即使我们将

00:08:52.149 --> 00:08:54.652
所有真实世界的数据都提供给模型，

00:08:54.652 --> 00:08:56.586
当我们也提供模拟数据时，

00:08:56.586 --> 00:08:59.055
我们仍然能够看到性能的进步，

00:08:59.055 --> 00:09:03.316
这意味着我们在这个抓握问题上
尚未达到数据容量限制。

00:09:03.824 --> 00:09:09.709
最后，有一种方法可以在
没有真实世界标签的情况下训练此设置，

00:09:09.859 --> 00:09:11.762
当我们在此环境中训练模型时，

00:09:11.762 --> 00:09:16.894
我们发现我们仍然能够在现实世界的机器人上
获得相当不错的性能。

00:09:19.091 --> 00:09:23.971
现在，这是 X 和 Brain 大型团队的工作成果。

00:09:24.170 --> 00:09:26.406
我要感谢与我合作的所有人。

00:09:26.406 --> 00:09:28.641
这是原始文件的链接。

00:09:28.641 --> 00:09:34.262
如果人们有兴趣了解更多的细节，
可以看看所提供的一篇博文。

00:09:34.565 --> 00:09:35.675
谢谢。

